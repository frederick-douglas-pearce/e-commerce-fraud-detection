{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# E-Commerce Fraud Detection - Modeling\n",
    "\n",
    "This notebook trains and evaluates machine learning models for fraud detection using the engineered features from `ec_fraud_detection.ipynb`.\n",
    "\n",
    "## Project Goal\n",
    "Deploy an optimally trained classification model capable of identifying fraudulent transactions with high precision and recall.\n",
    "\n",
    "## Modeling Approach\n",
    "1. Load pre-engineered features from EDA notebook\n",
    "2. Model-specific preprocessing (one-hot encoding, scaling)\n",
    "3. Baseline model training (Logistic Regression, Random Forest, XGBoost)\n",
    "4. Hyperparameter tuning\n",
    "5. Model evaluation with fraud-appropriate metrics\n",
    "6. Final model selection\n",
    "\n",
    "## Key Challenges\n",
    "- **Class Imbalance**: 44:1 ratio (97.8% normal, 2.2% fraud)\n",
    "- **Metric Selection**: ROC-AUC, F1, Precision-Recall (not accuracy)\n",
    "- **Business Trade-off**: Balance false positives (customer friction) vs false negatives (fraud losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "train_path = \"data/train_features.pkl\"\n",
    "val_path = \"data/val_features.pkl\"\n",
    "test_path = \"data/test_features.pkl\"\n",
    "\n",
    "# Target column\n",
    "target_col = \"is_fraud\"\n",
    "\n",
    "# Random seed for reproducibility\n",
    "random_seed = 1\n",
    "\n",
    "# Model output directory\n",
    "model_dir = \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Sklearn models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Sklearn metrics\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    f1_score, \n",
    "    precision_score, \n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "# Sklearn model selection\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "print(\"✓ All packages imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_header",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Load the pre-engineered datasets with final selected features from the EDA notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load engineered datasets\n",
    "train_df = pd.read_pickle(train_path)\n",
    "val_df = pd.read_pickle(val_path)\n",
    "test_df = pd.read_pickle(test_path)\n",
    "\n",
    "print(\"✓ Datasets loaded successfully\")\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"  • Training:   {train_df.shape}\")\n",
    "print(f\"  • Validation: {val_df.shape}\")\n",
    "print(f\"  • Test:       {test_df.shape}\")\n",
    "print(f\"\\nTotal features: {train_df.shape[1] - 1} (excluding target)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "### Inspect loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"Training data sample:\")\n",
    "display(train_df.head())\n",
    "\n",
    "# Check target distribution\n",
    "print(\"\\nTarget distribution (training set):\")\n",
    "fraud_rate = train_df[target_col].mean()\n",
    "print(train_df[target_col].value_counts())\n",
    "print(f\"\\nFraud rate: {fraud_rate:.4f} ({fraud_rate*100:.2f}%)\")\n",
    "print(f\"Class imbalance ratio: {(1-fraud_rate)/fraud_rate:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_types_header",
   "metadata": {},
   "source": [
    "### Identify feature types\n",
    "Categorize features for preprocessing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_types",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target from features\n",
    "feature_cols = [col for col in train_df.columns if col != target_col]\n",
    "\n",
    "# Identify numeric vs categorical features\n",
    "numeric_features = train_df[feature_cols].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = train_df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# For binary features that might be stored as int, we may want to treat them as categorical\n",
    "# Check for binary features in numeric columns\n",
    "binary_features = []\n",
    "for col in numeric_features:\n",
    "    unique_vals = train_df[col].nunique()\n",
    "    if unique_vals == 2:\n",
    "        binary_features.append(col)\n",
    "\n",
    "print(f\"Feature breakdown:\")\n",
    "print(f\"  • Total features: {len(feature_cols)}\")\n",
    "print(f\"  • Numeric features: {len(numeric_features)}\")\n",
    "print(f\"  • Categorical features: {len(categorical_features)}\")\n",
    "print(f\"  • Binary features (int encoded): {len(binary_features)}\")\n",
    "\n",
    "print(f\"\\nNumeric features ({len(numeric_features)}):\")\n",
    "print(f\"  {numeric_features}\")\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}):\")\n",
    "print(f\"  {categorical_features}\")\n",
    "\n",
    "if binary_features:\n",
    "    print(f\"\\nBinary features ({len(binary_features)}):\")\n",
    "    print(f\"  {binary_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "qhrpwv4fdy8",
   "source": "# Properly categorize features for model-specific preprocessing\n# Based on the 30 features from FraudFeatureTransformer\n\n# Continuous numeric features (need scaling for Logistic Regression)\ncontinuous_numeric = [\n    'account_age_days', 'total_transactions_user', 'avg_amount_user', \n    'amount', 'shipping_distance_km', 'hour_local', 'day_of_week_local',\n    'month_local', 'amount_deviation', 'amount_vs_avg_ratio', \n    'transaction_velocity', 'security_score'\n]\n\n# Categorical features (need encoding)\ncategorical = ['channel', 'promo_used', 'avs_match', 'cvv_result', 'three_ds_flag']\n\n# Binary features (already 0/1, no preprocessing needed)\nbinary = [\n    'is_weekend_local', 'is_late_night_local', 'is_business_hours_local',\n    'is_micro_transaction', 'is_large_transaction', 'is_new_account',\n    'is_high_frequency_user', 'country_mismatch', 'high_risk_distance',\n    'zero_distance', 'new_account_with_promo', 'late_night_micro_transaction',\n    'high_value_long_distance'\n]\n\nprint(\"Feature categorization for preprocessing:\")\nprint(f\"  • Continuous numeric: {len(continuous_numeric)}\")\nprint(f\"  • Categorical: {len(categorical)}\")\nprint(f\"  • Binary: {len(binary)}\")\nprint(f\"  • Total: {len(continuous_numeric) + len(categorical) + len(binary)}\")\n\n# Verify all 30 features are accounted for\nall_features = continuous_numeric + categorical + binary\nassert len(all_features) == 30, f\"Expected 30 features, got {len(all_features)}\"\nprint(\"\\n✓ All 30 features categorized correctly\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing_header",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Apply model-specific preprocessing transformations."
   ]
  },
  {
   "cell_type": "code",
   "id": "uhd8rfnz92",
   "source": "# Create preprocessing pipelines for different model types\n\nfrom sklearn.pipeline import Pipeline\n\n# For Logistic Regression: Scale numeric + One-hot encode categorical\nlogistic_preprocessor = ColumnTransformer([\n    ('num', StandardScaler(), continuous_numeric),\n    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical),\n    ('binary', 'passthrough', binary)\n], remainder='drop')\n\n# For tree-based models: Simple ordinal encoding (optional, trees can handle categoricals)\n# Using OrdinalEncoder for categorical features, passthrough for rest\nfrom sklearn.preprocessing import OrdinalEncoder\n\ntree_preprocessor = ColumnTransformer([\n    ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical),\n    ('rest', 'passthrough', continuous_numeric + binary)\n], remainder='drop')\n\nprint(\"✓ Preprocessing pipelines created:\")\nprint(\"  • Logistic Regression: StandardScaler + OneHotEncoder\")\nprint(\"  • Tree-based models: OrdinalEncoder (minimal)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "baseline_header",
   "metadata": {},
   "source": [
    "## Baseline Models\n",
    "Train initial models to establish performance baselines."
   ]
  },
  {
   "cell_type": "code",
   "id": "pe997b5mlek",
   "source": "# Define evaluation function for fraud detection models\n\ndef evaluate_model(model, X, y, model_name=\"Model\", dataset_name=\"Validation\"):\n    \"\"\"\n    Evaluate classification model with fraud-appropriate metrics.\n    \n    Returns dict with all metrics.\n    \"\"\"\n    # Get predictions\n    y_pred = model.predict(X)\n    y_pred_proba = model.predict_proba(X)[:, 1]\n    \n    # Calculate metrics\n    metrics = {\n        'model': model_name,\n        'dataset': dataset_name,\n        'roc_auc': roc_auc_score(y, y_pred_proba),\n        'pr_auc': average_precision_score(y, y_pred_proba),\n        'f1': f1_score(y, y_pred),\n        'precision': precision_score(y, y_pred),\n        'recall': recall_score(y, y_pred),\n        'accuracy': (y_pred == y).mean()\n    }\n    \n    # Print results\n    print(f\"\\n{model_name} - {dataset_name} Set Performance:\")\n    print(\"=\" * 60)\n    print(f\"  ROC-AUC:    {metrics['roc_auc']:.4f}\")\n    print(f\"  PR-AUC:     {metrics['pr_auc']:.4f}\")\n    print(f\"  F1 Score:   {metrics['f1']:.4f}\")\n    print(f\"  Precision:  {metrics['precision']:.4f}\")\n    print(f\"  Recall:     {metrics['recall']:.4f}\")\n    print(f\"  Accuracy:   {metrics['accuracy']:.4f}\")\n    print(\"=\" * 60)\n    \n    # Confusion matrix\n    cm = confusion_matrix(y, y_pred)\n    print(f\"\\nConfusion Matrix:\")\n    print(f\"  TN: {cm[0, 0]:,}  |  FP: {cm[0, 1]:,}\")\n    print(f\"  FN: {cm[1, 0]:,}  |  TP: {cm[1, 1]:,}\")\n    \n    return metrics\n\nprint(\"✓ Evaluation function defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3opuyu13q0h",
   "source": "### Baseline 1: Logistic Regression\nLinear model with StandardScaler + OneHotEncoder. Handles class imbalance with `class_weight='balanced'`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "nq69bso9q98",
   "source": "# Train Logistic Regression baseline\nprint(\"Training Logistic Regression...\")\n\n# Separate features and target\nX_train = train_df.drop(columns=[target_col])\ny_train = train_df[target_col]\n\nX_val = val_df.drop(columns=[target_col])\ny_val = val_df[target_col]\n\n# Create pipeline\nlogistic_pipeline = Pipeline([\n    ('preprocessor', logistic_preprocessor),\n    ('classifier', LogisticRegression(\n        class_weight='balanced',  # Handle class imbalance\n        max_iter=1000,\n        random_state=random_seed\n    ))\n])\n\n# Train\nlogistic_pipeline.fit(X_train, y_train)\n\n# Evaluate\nlogistic_metrics = evaluate_model(\n    logistic_pipeline, \n    X_val, \n    y_val, \n    model_name=\"Logistic Regression\",\n    dataset_name=\"Validation\"\n)\n\nprint(\"\\n✓ Logistic Regression baseline trained\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "msg8xti2eho",
   "source": "### Baseline 2: Random Forest\nTree-based ensemble model with minimal preprocessing. Handles class imbalance with `class_weight='balanced'`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9jskw54l9ep",
   "source": "# Train Random Forest baseline\nprint(\"Training Random Forest...\")\n\n# Create pipeline\nrf_pipeline = Pipeline([\n    ('preprocessor', tree_preprocessor),\n    ('classifier', RandomForestClassifier(\n        n_estimators=100,\n        class_weight='balanced',  # Handle class imbalance\n        random_state=random_seed,\n        n_jobs=-1  # Use all cores\n    ))\n])\n\n# Train\nrf_pipeline.fit(X_train, y_train)\n\n# Evaluate\nrf_metrics = evaluate_model(\n    rf_pipeline, \n    X_val, \n    y_val, \n    model_name=\"Random Forest\",\n    dataset_name=\"Validation\"\n)\n\nprint(\"\\n✓ Random Forest baseline trained\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9xpaf5j3034",
   "source": "### Baseline 3: XGBoost\nGradient boosting model with minimal preprocessing. Handles class imbalance with `scale_pos_weight` (ratio of negative to positive class).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9to98qgw5o",
   "source": "# Train XGBoost baseline\nprint(\"Training XGBoost...\")\n\n# Calculate scale_pos_weight for class imbalance\n# scale_pos_weight = (# negative class) / (# positive class)\nscale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\nprint(f\"Class imbalance ratio: {scale_pos_weight:.1f}:1\")\nprint(f\"Using scale_pos_weight={scale_pos_weight:.1f}\")\n\n# Create pipeline\nxgb_pipeline = Pipeline([\n    ('preprocessor', tree_preprocessor),\n    ('classifier', xgb.XGBClassifier(\n        n_estimators=100,\n        scale_pos_weight=scale_pos_weight,  # Handle class imbalance\n        random_state=random_seed,\n        n_jobs=-1,  # Use all cores\n        eval_metric='logloss'  # Suppress warning\n    ))\n])\n\n# Train\nxgb_pipeline.fit(X_train, y_train)\n\n# Evaluate\nxgb_metrics = evaluate_model(\n    xgb_pipeline, \n    X_val, \n    y_val, \n    model_name=\"XGBoost\",\n    dataset_name=\"Validation\"\n)\n\nprint(\"\\n✓ XGBoost baseline trained\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0gzjizyn465u",
   "source": "### Baseline Model Comparison\nCompare all baseline models on key fraud detection metrics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pqfaa53277o",
   "source": "# Create comparison table\ncomparison_df = pd.DataFrame([logistic_metrics, rf_metrics, xgb_metrics])\ncomparison_df = comparison_df.set_index('model')\ncomparison_df = comparison_df.drop(columns=['dataset'])\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"BASELINE MODEL COMPARISON - Validation Set\")\nprint(\"=\"*80)\ndisplay(comparison_df.style.format({\n    'roc_auc': '{:.4f}',\n    'pr_auc': '{:.4f}',\n    'f1': '{:.4f}',\n    'precision': '{:.4f}',\n    'recall': '{:.4f}',\n    'accuracy': '{:.4f}'\n}).background_gradient(cmap='RdYlGn', subset=['roc_auc', 'pr_auc', 'f1']))\n\n# Identify best model for each metric\nprint(\"\\n\" + \"=\"*80)\nprint(\"Best Performing Model by Metric:\")\nprint(\"=\"*80)\nfor metric in ['roc_auc', 'pr_auc', 'f1', 'precision', 'recall']:\n    best_model = comparison_df[metric].idxmax()\n    best_value = comparison_df[metric].max()\n    print(f\"  {metric.upper():15s}: {best_model:20s} ({best_value:.4f})\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "q9c119q21ue",
   "source": "# Visualize model comparison\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Plot 1: Key metrics comparison (ROC-AUC, PR-AUC, F1)\nax = axes[0]\nmetrics_to_plot = ['roc_auc', 'pr_auc', 'f1']\ncomparison_df[metrics_to_plot].plot(kind='bar', ax=ax, color=['steelblue', 'coral', 'lightgreen'])\nax.set_title('Key Fraud Detection Metrics Comparison', fontsize=14, fontweight='bold')\nax.set_xlabel('Model', fontsize=12)\nax.set_ylabel('Score', fontsize=12)\nax.set_xticklabels(comparison_df.index, rotation=45, ha='right')\nax.legend(['ROC-AUC', 'PR-AUC', 'F1 Score'], loc='lower right')\nax.grid(axis='y', alpha=0.3)\nax.set_ylim([0, 1])\n\n# Plot 2: Precision vs Recall tradeoff\nax = axes[1]\nx = np.arange(len(comparison_df))\nwidth = 0.35\n\nbars1 = ax.bar(x - width/2, comparison_df['precision'], width, label='Precision', color='steelblue')\nbars2 = ax.bar(x + width/2, comparison_df['recall'], width, label='Recall', color='coral')\n\nax.set_title('Precision vs Recall Tradeoff', fontsize=14, fontweight='bold')\nax.set_xlabel('Model', fontsize=12)\nax.set_ylabel('Score', fontsize=12)\nax.set_xticks(x)\nax.set_xticklabels(comparison_df.index, rotation=45, ha='right')\nax.legend()\nax.grid(axis='y', alpha=0.3)\nax.set_ylim([0, 1])\n\n# Add value labels on bars\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.3f}',\n                ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n✓ Baseline model comparison complete\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v9n2mhnzusk",
   "source": "### Key Insights from Baseline Models\n\n**Observations:**\n- All models show reasonable performance on the highly imbalanced dataset (44:1 ratio)\n- Class imbalance handling (class_weight/scale_pos_weight) is working effectively\n- Tree-based models (Random Forest, XGBoost) typically outperform Logistic Regression on this data\n\n**Next Steps:**\n1. **Hyperparameter Tuning**: Optimize the best performing baseline model(s)\n2. **Threshold Optimization**: Tune prediction threshold to balance precision/recall based on business requirements\n3. **Feature Importance**: Analyze which features contribute most to fraud detection\n4. **Test Set Evaluation**: Final evaluation on held-out test set\n\n**Metric Selection Guide:**\n- **ROC-AUC**: Overall model discrimination ability (higher is better)\n- **PR-AUC**: Performance on imbalanced data (more important than ROC-AUC for fraud)\n- **F1 Score**: Balance between precision and recall\n- **Precision**: Minimize false positives (customer friction)\n- **Recall**: Catch as many frauds as possible (minimize losses)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "tuning_header",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "Optimize model parameters for best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation_header",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Comprehensive evaluation with fraud-appropriate metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_model_header",
   "metadata": {},
   "source": [
    "## Final Model Selection\n",
    "Select and save the best performing model for deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}