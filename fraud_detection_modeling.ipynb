{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# E-Commerce Fraud Detection - Modeling\n",
    "\n",
    "This notebook trains and evaluates machine learning models for fraud detection using the engineered features from `ec_fraud_detection.ipynb`.\n",
    "\n",
    "## Project Goal\n",
    "Deploy an optimally trained classification model capable of identifying fraudulent transactions with high precision and recall.\n",
    "\n",
    "## Modeling Approach\n",
    "1. Load pre-engineered features from EDA notebook\n",
    "2. Model-specific preprocessing (one-hot encoding, scaling)\n",
    "3. Baseline model training (Logistic Regression, Random Forest, XGBoost)\n",
    "4. Hyperparameter tuning\n",
    "5. Model evaluation with fraud-appropriate metrics\n",
    "6. Final model selection\n",
    "\n",
    "## Key Challenges\n",
    "- **Class Imbalance**: 44:1 ratio (97.8% normal, 2.2% fraud)\n",
    "- **Metric Selection**: ROC-AUC, F1, Precision-Recall (not accuracy)\n",
    "- **Business Trade-off**: Balance false positives (customer friction) vs false negatives (fraud losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "train_path = \"data/train_features.pkl\"\n",
    "val_path = \"data/val_features.pkl\"\n",
    "test_path = \"data/test_features.pkl\"\n",
    "\n",
    "# Target column\n",
    "target_col = \"is_fraud\"\n",
    "\n",
    "# Random seed for reproducibility\n",
    "random_seed = 1\n",
    "\n",
    "# Model output directory\n",
    "model_dir = \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Sklearn models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Sklearn metrics\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    f1_score, \n",
    "    precision_score, \n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "# Sklearn model selection\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "print(\"✓ All packages imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_header",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Load the pre-engineered datasets with final selected features from the EDA notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load engineered datasets\n",
    "train_df = pd.read_pickle(train_path)\n",
    "val_df = pd.read_pickle(val_path)\n",
    "test_df = pd.read_pickle(test_path)\n",
    "\n",
    "print(\"✓ Datasets loaded successfully\")\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"  • Training:   {train_df.shape}\")\n",
    "print(f\"  • Validation: {val_df.shape}\")\n",
    "print(f\"  • Test:       {test_df.shape}\")\n",
    "print(f\"\\nTotal features: {train_df.shape[1] - 1} (excluding target)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "### Inspect loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"Training data sample:\")\n",
    "display(train_df.head())\n",
    "\n",
    "# Check target distribution\n",
    "print(\"\\nTarget distribution (training set):\")\n",
    "fraud_rate = train_df[target_col].mean()\n",
    "print(train_df[target_col].value_counts())\n",
    "print(f\"\\nFraud rate: {fraud_rate:.4f} ({fraud_rate*100:.2f}%)\")\n",
    "print(f\"Class imbalance ratio: {(1-fraud_rate)/fraud_rate:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_types_header",
   "metadata": {},
   "source": [
    "### Identify feature types\n",
    "Categorize features for preprocessing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_types",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target from features\n",
    "feature_cols = [col for col in train_df.columns if col != target_col]\n",
    "\n",
    "# Identify numeric vs categorical features\n",
    "numeric_features = train_df[feature_cols].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = train_df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# For binary features that might be stored as int, we may want to treat them as categorical\n",
    "# Check for binary features in numeric columns\n",
    "binary_features = []\n",
    "for col in numeric_features:\n",
    "    unique_vals = train_df[col].nunique()\n",
    "    if unique_vals == 2:\n",
    "        binary_features.append(col)\n",
    "\n",
    "print(f\"Feature breakdown:\")\n",
    "print(f\"  • Total features: {len(feature_cols)}\")\n",
    "print(f\"  • Numeric features: {len(numeric_features)}\")\n",
    "print(f\"  • Categorical features: {len(categorical_features)}\")\n",
    "print(f\"  • Binary features (int encoded): {len(binary_features)}\")\n",
    "\n",
    "print(f\"\\nNumeric features ({len(numeric_features)}):\")\n",
    "print(f\"  {numeric_features}\")\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}):\")\n",
    "print(f\"  {categorical_features}\")\n",
    "\n",
    "if binary_features:\n",
    "    print(f\"\\nBinary features ({len(binary_features)}):\")\n",
    "    print(f\"  {binary_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing_header",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Apply model-specific preprocessing transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline_header",
   "metadata": {},
   "source": [
    "## Baseline Models\n",
    "Train initial models to establish performance baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tuning_header",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "Optimize model parameters for best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation_header",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Comprehensive evaluation with fraud-appropriate metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_model_header",
   "metadata": {},
   "source": [
    "## Final Model Selection\n",
    "Select and save the best performing model for deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
